---
# Source: llm-usvc/templates/configmap.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-usvc-config
  labels:
    helm.sh/chart: llm-usvc-0.8.0
    app.kubernetes.io/name: llm-usvc
    app.kubernetes.io/instance: llm-usvc
    app.kubernetes.io/version: "v0.8"
    app.kubernetes.io/managed-by: Helm
data:
  LLM_MODEL_SERVER_ENDPOINT: "http://llm-usvc-tgi"
  LLM_MODEL_SERVER: "tgi"
  HF_TOKEN: "insert-your-huggingface-token-here"
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
---
# Source: llm-usvc/templates/service.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: llm-usvc
  labels:
    helm.sh/chart: llm-usvc-0.8.0
    app.kubernetes.io/name: llm-usvc
    app.kubernetes.io/instance: llm-usvc
    app.kubernetes.io/version: "v0.8"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: llm-usvc
  selector:
    app.kubernetes.io/name: llm-usvc
    app.kubernetes.io/instance: llm-usvc
---
# Source: llm-usvc/templates/deployment.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-usvc
  labels:
    helm.sh/chart: llm-usvc-0.8.0
    app.kubernetes.io/name: llm-usvc
    app.kubernetes.io/instance: llm-usvc
    app.kubernetes.io/version: "v0.8"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-usvc
      app.kubernetes.io/instance: llm-usvc
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llm-usvc
        app.kubernetes.io/instance: llm-usvc
    spec:
      securityContext:
        {}
      initContainers:
        - name: wait-for-model-server
          image: alpine/curl
          command:
            - sh
            - -c
            - |
                if [ -z "$LLM_MODEL_SERVER_ENDPOINT" ]; then
                  echo "Environment variable LLM_MODEL_SERVER_ENDPOINT is not set. Skipping the init container.";
                else
                  until curl -s $LLM_MODEL_SERVER_ENDPOINT; do
                    echo "waiting for embedding model server $LLM_MODEL_SERVER_ENDPOINT to be ready...";
                    sleep 2;
                  done;
                fi;
      containers:
        - name: llm-usvc
          envFrom:
            - configMapRef:
                name: llm-usvc-config
            - configMapRef:
                name: extra-env-config
                optional: true
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          image: "opea/llm-tgi:latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: llm-usvc
              containerPort: 9000
              protocol: TCP
          volumeMounts:
            - mountPath: /tmp
              name: tmp
          livenessProbe:
            failureThreshold: 24
            httpGet:
              path: v1/health_check
              port: llm-usvc
            initialDelaySeconds: 5
            periodSeconds: 60
          readinessProbe:
            httpGet:
              path: v1/health_check
              port: llm-usvc
            initialDelaySeconds: 5
            periodSeconds: 60
          startupProbe:
            failureThreshold: 120
            httpGet:
              path: v1/health_check
              port: llm-usvc
            initialDelaySeconds: 5
            periodSeconds: 60
          resources:
            {}
      volumes:
        - name: tmp
          emptyDir: {}
