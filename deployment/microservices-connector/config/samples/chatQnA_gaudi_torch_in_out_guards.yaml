# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: gmc.opea.io/v1alpha3
kind: GMConnector
metadata:
  labels:
    app.kubernetes.io/name: gmconnector
    app.kubernetes.io/managed-by: kustomize
    gmc/platform: gaudi
  name: chatqa
  namespace: chatqa
spec:
  routerConfig:
    name: router
    serviceName: router-service
  nodes:
    root:
      routerType: Sequence
      steps:
      - name: Fingerprint
        internalService:
          serviceName: fgp-svc
          config:
            endpoint: /v1/system_fingerprint/append_arguments
      - name: Embedding
        data: $response
        dependency: Hard
        internalService:
          serviceName: embedding-svc
          config:
            endpoint: /v1/embeddings
            EMBEDDING_MODEL_SERVER_ENDPOINT: torchserve-embedding-svc
            EMBEDDING_MODEL_SERVER: "torchserve"
            EMBEDDING_CONNECTOR: "langchain"
      - name: TorchserveEmbedding
        dependency: Hard
        internalService:
          serviceName: torchserve-embedding-svc
          isDownstreamService: true
      - name: Retriever
        data: $response
        dependency: Hard
        internalService:
          serviceName: retriever-svc
          config:
            endpoint: /v1/retrieval
      - name: VectorDB
        dependency: Hard
        internalService:
          serviceName: redis-vector-db
          isDownstreamService: true
      - name: Reranking
        dependency: Hard
        data: $response
        internalService:
          serviceName: reranking-svc
          config:
            endpoint: /v1/reranking
            RERANKING_SERVICE_ENDPOINT: tei-reranking-svc
      - name: TeiReranking
        dependency: Hard
        internalService:
          serviceName: tei-reranking-svc
          config:
            endpoint: /rerank
          isDownstreamService: true
      - name: PromptTemplate
        dependency: Hard
        data: $response
        internalService:
          serviceName: prompt-template-svc
          config:
            endpoint: /v1/prompt_template
      - name: LLMGuardInput
        dependency: Hard
        data: $response
        internalService:
          serviceName: input-scan-svc
          config:
            endpoint: /v1/llmguardinput
      - name: VLLMGaudi
        dependency: Hard
        internalService:
          serviceName: vllm-gaudi-svc
          config:
            endpoint: /v1/completions
          isDownstreamService: true
      - name: Llm
        data: $response
        dependency: Hard
        internalService:
          serviceName: llm-svc
          config:
            endpoint: /v1/chat/completions
            LLM_MODEL_SERVER: vllm
            LLM_MODEL_SERVER_ENDPOINT: vllm-gaudi-svc
            LLM_OUTPUT_GUARD_EXISTS: "True"
      - name: LLMGuardOutput
        dependency: Hard
        data: $response
        internalService:
          serviceName: output-scan-svc
          config:
            endpoint: /v1/llmguardoutput
      - name: Tts
        data: $response
        dependency: Hard
        internalService:
          serviceName: tts-svc
          config:
            endpoint: /v1/tts
            TTS_MODEL_SERVER_ENDPOINT: torchserve-tts-svc
            TTS_MODEL_NAME: "microsoft/speecht5_tts"
            TTS_MODEL_SERVER: "torchserve"
            TTS_VOCODER_MODEL_NAME: "microsoft/speecht5_hifigan"
      - name: TorchserveTTS
        dependency: Hard
        internalService:
          serviceName: torchserve-tts-svc
          config:
            TORCHSERVE_MODEL_NAME: "microsoft/speecht5_tts"
            TORCHSERVE_VOCODER_MODEL_NAME: "microsoft/speecht5_hifigan"
          isDownstreamService: true
      - name: Asr
        data: $response
        dependency: Hard
        internalService:
          serviceName: asr-svc
          config:
            endpoint: /v1/asr
            ASR_MODEL_SERVER_ENDPOINT: torchserve-asr-svc
            ASR_MODEL_NAME: "openai/whisper-small"
            ASR_MODEL_SERVER: "torchserve"
            # ASR_VOCODER_MODEL_NAME: "microsoft/speecht5_hifigan"
      - name: TorchserveASR
        dependency: Hard
        internalService:
          serviceName: torchserve-asr-svc
          config:
            TORCHSERVE_MODEL_NAME: "openai/whisper-small"
            # TORCHSERVE_VOCODER_MODEL_NAME: "microsoft/speecht5_hifigan"
          isDownstreamService: true
