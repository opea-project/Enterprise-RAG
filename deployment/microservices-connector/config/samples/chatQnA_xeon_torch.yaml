# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: gmc.opea.io/v1alpha3
kind: GMConnector
metadata:
  labels:
    app.kubernetes.io/name: gmconnector
    app.kubernetes.io/managed-by: kustomize
    gmc/platform: xeon
  name: chatqa
  namespace: chatqa
spec:
  routerConfig:
    name: router
    serviceName: router-service
  nodes:
    root:
      routerType: Sequence
      steps:
      - name: Fingerprint
        internalService:
          serviceName: fgp-svc
          config:
            endpoint: /v1/system_fingerprint/append_arguments
      - name: Embedding
        data: $response
        internalService:
          serviceName: embedding-svc
          config:
            endpoint: /v1/embeddings
            EMBEDDING_MODEL_SERVER_ENDPOINT: torchserve-embedding-svc
            EMBEDDING_MODEL_NAME: "BAAI/bge-base-en-v1.5"
            EMBEDDING_MODEL_SERVER: "torchserve"
            EMBEDDING_CONNECTOR: "langchain"
      - name: TorchserveEmbedding
        internalService:
          serviceName: torchserve-embedding-svc
          config:
            TORCHSERVE_MODEL_NAME: "BAAI/bge-base-en-v1.5"
          isDownstreamService: true
      - name: Retriever
        data: $response
        internalService:
          serviceName: retriever-svc
          config:
            endpoint: /v1/retrieval
            EMBED_MODEL: "BAAI/bge-base-en-v1.5"
      - name: VectorDB
        internalService:
          serviceName: redis-vector-db
          isDownstreamService: true
      - name: Reranking
        data: $response
        internalService:
          serviceName: reranking-svc
          config:
            endpoint: /v1/reranking
            RERANKING_SERVICE_ENDPOINT: tei-reranking-svc
      - name: TeiReranking
        internalService:
          serviceName: tei-reranking-svc
          config:
            endpoint: /rerank
          isDownstreamService: true
      - name: PromptTemplate
        dependency: Hard
        data: $response
        internalService:
          serviceName: prompt-template-svc
          config:
            endpoint: /v1/prompt_template
      - name: VLLM
        internalService:
          serviceName: vllm-service-m
          config:
            endpoint: /v1/completions
          isDownstreamService: true
      - name: Llm
        data: $response
        internalService:
          serviceName: llm-svc
          config:
            endpoint: /v1/chat/completions
            LLM_MODEL_SERVER: vllm
            LLM_MODEL_SERVER_ENDPOINT: vllm-service-m
