# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
# User-configurable parameters (can be set via --set during helm install)


modelConfigs:
  generic-base-cpu: &generic_base_cpu
    configMapValues:
      VLLM_SKIP_WARMUP: "false"
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_DTYPE: "bfloat16"
      VLLM_MAX_NUM_SEQS: "256"
      VLLM_TP_SIZE: "1"
      OMP_NUM_THREADS: "32"
      VLLM_PP_SIZE: "1"
      VLLM_MAX_MODEL_LEN: "4096"
    extraCmdArgs: ["--device", "cpu", "--pipeline-parallel-size", "$(VLLM_PP_SIZE)", "--dtype", "$(VLLM_DTYPE)", "--max_model_len", "$(VLLM_MAX_MODEL_LEN)", "--max-num-seqs", "$(VLLM_MAX_NUM_SEQS)", "--disable-log-requests", "--download-dir", "/data"]
    tensor_parallel_size: "1" # not applied on CPU

  generic-base-awq-cpu: &generic_base_awq_cpu
    configMapValues:
      VLLM_SKIP_WARMUP: "false"
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_DTYPE: "bfloat16"
      VLLM_MAX_NUM_SEQS: "256"
      VLLM_TP_SIZE: "1"
      OMP_NUM_THREADS: "32"
      VLLM_PP_SIZE: "1"
      VLLM_MAX_MODEL_LEN: "4096"
    extraCmdArgs: ["--device", "cpu", "--pipeline-parallel-size", "$(VLLM_PP_SIZE)", "--dtype", "$(VLLM_DTYPE)", "--max_model_len", "$(VLLM_MAX_MODEL_LEN)", "--max-num-seqs", "$(VLLM_MAX_NUM_SEQS)", "--disable-log-requests", "--download-dir", "/data", "--quantization", "awq", "--enforce-eager"]
    tensor_parallel_size: "1" # not applied on CPU

  "TechxGenus/Meta-Llama-3-8B-Instruct-AWQ":
    <<: *generic_base_awq_cpu

  "solidrust/Llama-3-13B-Instruct-v0.1-AWQ":
    <<: *generic_base_awq_cpu

  "MaziyarPanahi/Llama-3-13B-Instruct-v0.1":
    <<: *generic_base_cpu

  "TheBloke/neural-chat-7B-v3-3-AWQ":
    <<: *generic_base_awq_cpu

  "Intel/neural-chat-7b-v3-3":
    <<: *generic_base_cpu

  "casperhansen/llama-3-8b-instruct-awq":
    <<: *generic_base_awq_cpu

  "meta-llama/Llama-3.1-8B-Instruct":
    <<: *generic_base_cpu

  "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4":
    <<: *generic_base_awq_cpu

  "haoranxu/ALMA-7B-R":
    <<: *generic_base_cpu
    modelChatTemplate: |
      {%- for message in messages if message['role'] != 'system' -%}
        {%- if message['role'] == 'user' -%}
          {{- "<s>[INST] " + message['content'].strip() + " [/INST]" -}}
        {%- elif message['role'] == 'assistant' -%}
          {{- " " + message['content'].strip() + " </s>" -}}
        {%- endif -%}
      {%- endfor -%}
      {%- if add_generation_prompt -%}
        <s>[INST]
      {%- endif -%}

  "Qwen/Qwen3-14B":
    <<: *generic_base_cpu

  "Qwen/Qwen3-14B-AWQ":
    <<: *generic_base_awq_cpu

  "mistralai/Mistral-7B-Instruct-v0.3":
    <<: *generic_base_cpu

  "solidrust/Mistral-7B-Instruct-v0.3-AWQ":
    <<: *generic_base_awq_cpu
    modelChatTemplate: |
      {%- if messages[0]['role'] == 'system' %}
        {%- set system_message = messages[0]['content'] %}
        {%- set loop_messages = messages[1:] %}\n{%- else %}
        {%- set loop_messages = messages %}
      {%- endif %}
      {{- bos_token }}
      {%- for message in loop_messages %}
        {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}
          {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}
        {%- endif %}
        {%- if message['role'] == 'user' %}
          {%- if loop.first and system_message is defined %}
            {{- ' [INST] ' + system_message + '\\n\\n' + message['content'] + ' [/INST]' }}
          {%- else %}
            {{- ' [INST] ' + message['content'] + ' [/INST]' }}
          {%- endif %}
        {%- elif message['role'] == 'assistant' %}
          {{- ' ' + message['content'] + eos_token}}
      {%- else %}
        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}
      {%- endif %}
      {%- endfor %}

# Default arguments if the model ID is not found
defaultModelConfigs:
  configMapValues:
    OMPI_MCA_btl_vader_single_copy_mechanism: none
    VLLM_CPU_KVCACHE_SPACE: "40"
  extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len", "5196"]
  modelChatTemplate: ""     # default is empty
  tensor_parallel_size: "1" # not applied on CPU

