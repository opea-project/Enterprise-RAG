# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
# User-configurable parameters (can be set via --set during helm install)


modelConfigs:
  generic-base-cpu: &generic_base_cpu
    configMapValues:
      VLLM_SKIP_WARMUP: "false"
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_DTYPE: "bfloat16"
      VLLM_MAX_NUM_SEQS: "256"
      VLLM_TP_SIZE: "1"
      OMP_NUM_THREADS: "32"
      VLLM_PP_SIZE: "1"
      VLLM_MAX_MODEL_LEN: "4096"
    extraCmdArgs: ["--device", "cpu", "--pipeline-parallel-size", "$(VLLM_PP_SIZE)", "--dtype", "$(VLLM_DTYPE)", "--max_model_len", "$(VLLM_MAX_MODEL_LEN)", "--max-num-seqs", "$(VLLM_MAX_NUM_SEQS)", "--disable-log-requests", "--download-dir", "/data"]
    tensor_parallel_size: "1" # not applied on CPU

  generic-base-awq-cpu: &generic_base_awq_cpu
    configMapValues:
      VLLM_SKIP_WARMUP: "false"
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_DTYPE: "bfloat16"
      VLLM_MAX_NUM_SEQS: "256"
      VLLM_TP_SIZE: "1"
      OMP_NUM_THREADS: "32"
      VLLM_PP_SIZE: "1"
      VLLM_MAX_MODEL_LEN: "4096"
    extraCmdArgs: ["--device", "cpu", "--pipeline-parallel-size", "$(VLLM_PP_SIZE)", "--dtype", "$(VLLM_DTYPE)", "--max_model_len", "$(VLLM_MAX_MODEL_LEN)", "--max-num-seqs", "$(VLLM_MAX_NUM_SEQS)", "--disable-log-requests", "--download-dir", "/data", "--quantization", "awq"]
    tensor_parallel_size: "1" # not applied on CPU

  "TechxGenus/Meta-Llama-3-8B-Instruct-AWQ":
    <<: *generic_base_awq_cpu

  "solidrust/Llama-3-13B-Instruct-v0.1-AWQ":
    <<: *generic_base_awq_cpu

  "MaziyarPanahi/Llama-3-13B-Instruct-v0.1":
    <<: *generic_base_cpu

  "TheBloke/neural-chat-7B-v3-3-AWQ":
    <<: *generic_base_awq_cpu

  "Intel/neural-chat-7b-v3-3":
    <<: *generic_base_cpu

  "casperhansen/llama-3-8b-instruct-awq":
    <<: *generic_base_awq_cpu

  "meta-llama/Llama-3.1-8B-Instruct":
    <<: *generic_base_cpu

  "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4":
    <<: *generic_base_awq_cpu

  "haoranxu/ALMA-7B-R":
    <<: *generic_base_cpu
    modelChatTemplate: |
      {%- for message in messages if message['role'] != 'system' -%}
        {%- if message['role'] == 'user' -%}
          {{- "<s>[INST] " + message['content'].strip() + " [/INST]" -}}
        {%- elif message['role'] == 'assistant' -%}
          {{- " " + message['content'].strip() + " </s>" -}}
        {%- endif -%}
      {%- endfor -%}
      {%- if add_generation_prompt -%}
        <s>[INST]
      {%- endif -%}



# Default arguments if the model ID is not found
defaultModelConfigs:
  configMapValues:
    OMPI_MCA_btl_vader_single_copy_mechanism: none
    VLLM_CPU_KVCACHE_SPACE: "40"
  extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len", "5196"]
  modelChatTemplate: ""     # default is empty
  tensor_parallel_size: "1" # not applied on CPU

