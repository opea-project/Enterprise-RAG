# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
# User-configurable parameters (can be set via --set during helm install)
accelDevice: "gaudi"

block_size: 128          # Default KV cache block size
max_num_seqs: 256        # Default maximum number of sequences
max_seq_len_to_capture: 2048 # Default maximum sequence length
d_type: "bfloat16"
max_model_len: 5120

image:
  repository: opea/vllm-gaudi
  tag: "1.2.1"
  pullPolicy: IfNotPresent


# Generic Model Configuration
runtime: "habana"
HABANA_VISIBLE_DEVICES: "all"
VLLM_NO_USAGE_STATS: 1
DO_NOT_TRACK: 1

#FP8 Quantization Configuration
# EXPERIMENTAL_WEIGHT_SHARING: "0"
# VLLM_SKIP_WARMUP: "true"
# QUANT_CONFIG: "/data/41bd4c9e7e4fb318ca40e721131d4933966c2cc1/maxabs_quant_g2.json"
# extraCmdArgs: ["--quantization","inc","--kv-cache-dtype","fp8_inc", "--weights-load-device", "cpu", "--max-model-len","8192", "--max-num-seq","256", "--max-num-batched-tokens","8192"]


# # Dnver Configuration for 128K
# PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
# VLLM_GRAPH_RESERVED_MEM: "0.2"
# extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-num-seqs","256","--max-seq_len-to-capture","2048","--swap-space","6","--num-scheduler-steps","2","--gpu-memory-utilization","0.8"]


# Partha's Configuration for 128K
# EXPERIMENTAL_WEIGHT_SHARING: "0"
# VLLM_GRAPH_RESERVED_MEM: "0.22"
# VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
# VLLM_PROMPT_SEQ_BUCKET_MAX: "131328"
# VLLM_DECODE_BLOCK_BUCKET_MAX: "20520"
# VLLM_PROMPT_BS_BUCKET_STEP: "4"
# VLLM_DECODE_BS_BUCKET_STEP: "4"
# VLLM_PROMPT_SEQ_BUCKET_STEP: "1024"
# VLLM_GRAPH_PROMPT_RATIO: "0.4"
# VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
# PT_HPU_RECIPE_CACHE_CONFIG: "./recipe_cache,true,1024"
# extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","131328","--gpu-memory-util","0.99","--max-num-seqs","20","--max-num-prefill-seqs","1","--num_scheduler_steps","16","--use-padding-aware-scheduling"]


LLM_MODEL_ID: ""

modelConfigs:
  # Qwen/Qwen2.5-7B-Instruct
  "Qwen/Qwen2.5-7B-Instruct":
    configMapValues:
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      VLLM_CPU_KVCACHE_SPACE: "40"
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
    tensor_parallel_size: "1"

  # Qwen/Qwen2.5-14B-Instruct
  "Qwen/Qwen2.5-14B-Instruct":
    configMapValues:
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      VLLM_CPU_KVCACHE_SPACE: "40"
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
    tensor_parallel_size: "2"

  # Qwen/Qwen2.5-32B-Instruct
  "Qwen/Qwen2.5-32B-Instruct":
    configMapValues:
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      VLLM_CPU_KVCACHE_SPACE: "40"
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
    tensor_parallel_size: "2"

  # Qwen/Qwen2.5-72B-Instruct
  "Qwen/Qwen2.5-72B-Instruct":
    configMapValues:
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      VLLM_CPU_KVCACHE_SPACE: "40"
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
    tensor_parallel_size: "4"

  # meta-llama/Meta-Llama-3.1-8B-Instruct
  "meta-llama/Meta-Llama-3.1-8B-Instruct":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"

  # meta-llama/Llama-3.1-70B-Instruct
  "meta-llama/Llama-3.1-70B-Instruct":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"

  # meta-llama/Llama-3.1-405B-Instruct-FP8
  "meta-llama/Llama-3.1-405B-Instruct-FP8":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "true"
      QUANT_CONFIG: "/data/41bd4c9e7e4fb318ca40e721131d4933966c2cc1/maxabs_quant_g2.json"
    extraCmdArgs: ["--quantization","inc","--kv-cache-dtype","fp8_inc", "--weights-load-device", "cpu", "--max-model-len","33024", "--max-num-seq","256", "--max-num-batched-tokens","8192"]
    tensor_parallel_size: "8"

  # meta-llama/Llama-3.2-1B-Instruct
  "meta-llama/Llama-3.2-1B-Instruct":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "24768"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","96","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"

  # meta-llama/Llama-3.2-3B-Instruct
  "meta-llama/Llama-3.2-3B-Instruct":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"

  # meta-llama/Llama-3.3-70B-Instruct
  "meta-llama/Llama-3.3-70B-Instruct":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "4352"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "9792"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","288","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"
    #   PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
    #   EXPERIMENTAL_WEIGHT_SHARING: "0"
    #   VLLM_GRAPH_RESERVED_MEM: "0.22"
    #   VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
    #   VLLM_PROMPT_SEQ_BUCKET_MAX: "131328"
    #   VLLM_DECODE_BLOCK_BUCKET_MAX: "20520"
    #   VLLM_PROMPT_BS_BUCKET_STEP: "4"
    #   VLLM_DECODE_BS_BUCKET_STEP: "4"
    #   VLLM_PROMPT_SEQ_BUCKET_STEP: "1024"
    #   VLLM_GRAPH_PROMPT_RATIO: "0.4"
    #   VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
    #   PT_HPU_RECIPE_CACHE_CONFIG: "/data/.recipe_cache,false,8192"

    # extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","131328","--gpu-memory-util","0.99","--max-num-seqs","20","--max-num-prefill-seqs","1","--num_scheduler_steps","16","--use-padding-aware-scheduling"]
    # # Dnver Configuration for 128K
    # PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
    # VLLM_GRAPH_RESERVED_MEM: "test"
    # extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-num-seqs","256","--max-seq_len-to-capture","2048","--swap-space","6","--num-scheduler-steps","2","--gpu-memory-utilization","0.8"]

  # mistralai/Mistral-7B-Instruct-v0.2
  "mistralai/Mistral-7B-Instruct-v0.2":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling"]
    tensor_parallel_size: "1"

  # mistralai/Mistral-7B-Instruct-v0.1
  "mistralai/Mistral-7B-Instruct-v0.1":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling"]
    tensor_parallel_size: "1"

  # mistralai/Mixtral-8x7B-Instruct-v0.1
  "mistralai/Mixtral-8x7B-Instruct-v0.1":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      PT_HPU_LAZY_MODE: "1"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "true"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_ENABLE_EXPERT_PARALLEL: "0"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling"]
    tensor_parallel_size: "2"

  # tiiuae/Falcon3-7B-Instruct
  "tiiuae/Falcon3-7B-Instruct":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      # LD_PRELOAD: "/usr/lib/x86_64-linux-gnu/libjemalloc.so.2"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling"]
    tensor_parallel_size: "1"

  # tiiuae/Falcon3-10B-Instruct
  "tiiuae/Falcon3-10B-Instruct":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      # LD_PRELOAD: "/usr/lib/x86_64-linux-gnu/libjemalloc.so.2"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling"]
    tensor_parallel_size: "1"

  # deepseek-ai/DeepSeek-R1
  "deepseek-ai/DeepSeek-R1":
    configMapValues:
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      VLLM_CPU_KVCACHE_SPACE: "40"
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
    tensor_parallel_size: "8"

  # deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  "deepseek-ai/DeepSeek-R1-Distill-Llama-8B":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"

  # deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"

  # deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B":
    configMapValues:
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      VLLM_CPU_KVCACHE_SPACE: "40"
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
    tensor_parallel_size: "2"

  # iternal/Meta-Llama-3-8B-Instruct
  "iternal/Meta-Llama-3-8B-Instruct":
    configMapValues:
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      VLLM_CPU_KVCACHE_SPACE: "40"
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/.recipe_cache,false,8192"
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
    tensor_parallel_size: "1"

# Default arguments if the model ID is not found
defaultModelConfigs:
  configMapValues:
    OMPI_MCA_btl_vader_single_copy_mechanism: none
    PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
    VLLM_CPU_KVCACHE_SPACE: "40"
  extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
  tensor_parallel_size: "1"
