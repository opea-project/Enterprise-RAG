# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# resources-reference-cpu.yaml is a file that contains the resource requests and limits for each microservice in the microservices-connector deployment.

services:
  embedding-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 4Gi
  fingerprint-usvc:
    replicas: 1
    resources:
      requests:
        memory: 64Mi
        cpu: 0.25
      limits:
        memory: 1Gi
        cpu: 0.5
  in-guard-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 4
        memory: 8Gi
      limits:
        cpu: 8
        memory: 16Gi
    hpa:
      minReplicas: 1
      maxReplicas: 3
      AverageValue: 1
      # this is average time of request processing in input guardrails service.
      # this time is dependent on enabled scanners and # of requests
      # metric used for scaling avg(rate(http_request_duration_seconds_sum{service="input-scan-svc", namespace="chatqa", handler!~"/metrics|/v1/health_check|none"}[1m])) by (<<.GroupBy>>) / (0.001 + avg(rate(http_request_duration_seconds_count{service="input-scan-svc", namespace="chatqa", handler!~"/metrics|/v1/health_check|none"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 120
          policies:
          - type: Percent
            value: 25
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 60
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 120
          policies:
          - type: Percent
            value: 25
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 60
  llm-usvc:
    replicas: 2
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 6Gi
  out-guard-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 4
        memory: 8Gi
      limits:
        cpu: 8
        memory: 16Gi
    hpa:
      minReplicas: 1
      maxReplicas: 3
      AverageValue: 5
      # this is average time of request processing in output guardrails service.
      # this time is dependent on enabled scanners and # of requests
      # metric used for scaling 'avg(rate(http_request_duration_seconds_sum{service="output-scan-svc", namespace="chatqa", handler!~"/metrics|/v1/health_check|none"}[1m])) by (<<.GroupBy>>) / (0.001 + avg(rate(http_request_duration_seconds_count{service="output-scan-svc", namespace="chatqa", handler!~"/metrics|/v1/health_check|none"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 60
          policies:
          - type: Percent
            value: 25
            periodSeconds: 120
          - type: Pods
            value: 1
            periodSeconds: 120
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 60
          policies:
          - type: Percent
            value: 25
            periodSeconds: 120
          - type: Pods
            value: 1
            periodSeconds: 120
  prompt-template-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 2Gi
  reranking-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 2Gi
  retriever-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 6
        memory: 6Gi
  teirerank:
    replicas: 1
    resources:
      requests:
        cpu: 2
        memory: 16Gi
      limits:
        cpu: 8
        memory: 32Gi
    hpa:
      minReplicas: 1
      maxReplicas: 3
      AverageValue: 2
      # this is average time of processing singlr request in re-ranker
      # depends on hardware used and number of requests
      # metric used: 'sum(rate(te_request_duration_sum{namespace="chatqa",service="tei-reranking-svc"}[1m])) by (<<.GroupBy>>) / (0.001 + sum(rate(te_request_duration_count{namespace="chatqa",service="tei-reranking-svc"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 120
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 0
          policies:
          - type: Percent
            value: 50
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 15
  tei:
    replicas: 1
    resources:
      requests:
        cpu: 8
        memory: 8Gi
      limits:
        cpu: 16
        memory: 16Gi
  torchserve_embedding:
    replicas: 4
    resources:
      requests:
        cpu: 4
        memory: 4Gi
      limits:
        cpu: 4
        memory: 16Gi
    hpa:
      minReplicas: 4
      maxReplicas: 10
      targetValue: 0.5
      # this value represents average processing time of single request (Queue Latency + interference latency)
      # will increase when embedding documents with high number of chunks
      # depends on used hardware and embedding model used
      # metric used for scale up 'avg(rate(ts_inference_requests_total{namespace="chatqa",service="torchserve-embedding-svc"}[2m])) by (<<.GroupBy>>)'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 30
          - type: Pods
            value: 1
            periodSeconds: 15
        scaleUp:
          stabilizationWindowSeconds: 30
          selectPolicy: Max
          policies:
          - type: Pods
            value: 1
            periodSeconds: 15
  torchserve_reranking:
    replicas: 2
    resources:
      requests:
        cpu: 4
        memory: 4Gi
      limits:
        cpu: 4
        memory: 16Gi
    hpa:
      minReplicas: 2
      maxReplicas: 3
      targetValue: 1000m
      # this is average requests queue latency in miliseconds
      # depends on hardware used and number of requests
      # metric used: 'avg(rate(ts_queue_latency_microseconds{namespace="chatqa",service="torchserve-reranking-svc"}[1m])) by (<<.GroupBy>>) / 1000000'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 120
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 0
          policies:
          - type: Percent
            value: 50
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 15
  vllm:
    replicas: 1
    # NOTE:
    # The resources block below is used for vllm ONLY if the balloons policy is NOT enabled.
    # If balloons policy is enabled, vllm resources must be defined in the inventory config.yaml as balloons.services.vllm.resources
    resources:
      requests:
        cpu: 32
        memory: 64Gi
      limits:
        cpu: 32
        memory: 100Gi
    hpa:
      minReplicas: 1
      maxReplicas: 2
        # setting max replicas to 2 as to get good performance numbers we need either to:
        #  use multinode and spread vllm instances accross the nodes or to pin the cores/memory to socket
      targetValue: 100m
      # average processing time of single token
      # 1/target value would mean tokens/sec
      # depends on used LLM model and hardware platform used
      # 'sum(rate(vllm:time_per_output_token_seconds_sum{service="vllm-service-m"}[1m])) by (<<.GroupBy>>) / (0.001 + sum(rate(vllm:time_per_output_token_seconds_count{service="vllm-service-m"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 90
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 60
          policies:
          - type: Pods
            value: 1
            periodSeconds: 90
