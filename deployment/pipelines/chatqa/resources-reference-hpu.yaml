# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# resources-reference-hpu.yaml is a file that contains the resource requests and limits for each microservice in the microservices-connector deployment.

services:
  embedding-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 4Gi
  fingerprint-usvc:
    replicas: 1
    resources:
      requests:
        memory: 64Mi
        cpu: 0.25
      limits:
        memory: 1Gi
        cpu: 0.5
  in-guard-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 4
        memory: 8Gi
      limits:
        cpu: 8
        memory: 16Gi
    hpa:
      minReplicas: 1
      maxReplicas: 3
      AverageValue: 1
      # this is average time of request processing in input guardrails service.
      # this time is dependent on enabled scanners and # of requests
      # metric used for scaling avg(rate(http_request_duration_seconds_sum{service="input-scan-svc", namespace="chatqa", handler!~"/metrics|/v1/health_check|none"}[1m])) by (<<.GroupBy>>) / (0.001 + avg(rate(http_request_duration_seconds_count{service="input-scan-svc", namespace="chatqa", handler!~"/metrics|/v1/health_check|none"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 60
          policies:
          - type: Percent
            value: 25
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 15
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 30
          policies:
          - type: Percent
            value: 50
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 15
  llm-usvc:
    replicas: 2
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 6Gi
  out-guard-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 4
        memory: 8Gi
      limits:
        cpu: 8
        memory: 16Gi
    hpa:
      minReplicas: 1
      maxReplicas: 3
      AverageValue: 1
      # this is average time of request processing in output guardrails service.
      # this time is dependent on enabled scanners and # of requests
      # metric used for scaling 'avg(rate(http_request_duration_seconds_sum{service="output-scan-svc", namespace="chatqa", handler!~"/metrics|/v1/health_check|none"}[1m])) by (<<.GroupBy>>) / (0.001 + avg(rate(http_request_duration_seconds_count{service="output-scan-svc", namespace="chatqa", handler!~"/metrics|/v1/health_check|none"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 60
          policies:
          - type: Percent
            value: 25
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 15
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 30
          policies:
          - type: Percent
            value: 50
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 15
  prompt-template-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 2Gi
  reranking-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 2Gi
  retriever-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 8
        memory: 8Gi
  tei_gaudi:
    replicas: 1
    resources:
      limits:
        habana.ai/gaudi: 1
  teirerank:
    replicas: 1
    resources:
      requests:
        cpu: 2
        memory: 16Gi
      limits:
        cpu: 8
        memory: 32Gi
    hpa:
      minReplicas: 1
      maxReplicas: 3
      AverageValue: 2
      # this is average time of processing singlr request in re-ranker
      # depends on hardware used and number of requests
      # metric used: 'sum(rate(te_request_duration_sum{namespace="chatqa",service="tei-reranking-svc"}[1m])) by (<<.GroupBy>>) / (0.001 + sum(rate(te_request_duration_count{namespace="chatqa",service="tei-reranking-svc"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 120
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 0
          policies:
          - type: Percent
            value: 50
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 15
  teirerank_gaudi:
    replicas: 1
    resources:
      limits:
        habana.ai/gaudi: 1
  tei:
    replicas: 1
    resources:
      requests:
        cpu: 8
        memory: 8Gi
      limits:
        cpu: 16
        memory: 16Gi
    hpa:
      minReplicas: 4
      maxReplicas: 10
      targetValue: 0.5
      # this value represents average processing time of single request (Queue Latency + interference latency)
      # will increase when embedding documents with high number of chunks
      # depends on used hardware and embedding model used
      # metric used for scale up 'avg(rate(ts_inference_requests_total{namespace="chatqa",service="torchserve-embedding-svc"}[2m])) by (<<.GroupBy>>)'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 30
          - type: Pods
            value: 1
            periodSeconds: 15
        scaleUp:
          stabilizationWindowSeconds: 30
          selectPolicy: Max
          policies:
          - type: Pods
            value: 1
            periodSeconds: 15
  torchserve_reranking:
    replicas: 2
    resources:
      requests:
        cpu: 4
        memory: 4Gi
      limits:
        cpu: 4
        memory: 16Gi
    hpa:
      minReplicas: 2
      maxReplicas: 3
      targetValue: 1000m
      # this is average requests queue latency in miliseconds
      # depends on hardware used and number of requests
      # metric used: 'avg(rate(ts_queue_latency_microseconds{namespace="chatqa",service="torchserve-reranking-svc"}[1m])) by (<<.GroupBy>>) / 1000000'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 120
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 0
          policies:
          - type: Percent
            value: 50
            periodSeconds: 15
          - type: Pods
            value: 1
            periodSeconds: 15
  vllm_gaudi:
    replicas: 1
    # Note: For vllm_gaudi, the habana.ai/gaudi limit is set to the tensor-parallel-size value defined in deployment/pipelines/chatqa/resources-model-hpu.yaml
    hpa:
      minReplicas: 1
      maxReplicas: 4
      targetValue: 40m
      # average processing time of single token
      # 1/target value would mean tokens/sec
      # depends on used LLM model
      # metric used for scaling 'sum(rate(vllm:time_per_output_token_seconds_sum{service="vllm-gaudi-svc"}[1m])) by (<<.GroupBy>>) / (0.001 + sum(rate(vllm:time_per_output_token_seconds_count{service="vllm-gaudi-svc"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 90
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 0
          policies:
          - type: Pods
            value: 1
            periodSeconds: 150
