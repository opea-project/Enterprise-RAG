# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# resources-reference-hpu.yaml is a file that contains the resource requests and limits for each microservice in the microservices-connector deployment.

services:
  text-extractor-usvc:
    replicas: 2
    resources:
      requests:
        cpu: 4
        memory: 1Gi
      limits:
        cpu: 8
        memory: 8Gi
  text-compression-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 0.5
        memory: 64Mi
      limits:
        cpu: 2
        memory: 1Gi
  text-splitter-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 1Gi
      limits:
        cpu: 4
        memory: 2Gi
  llm-usvc:
    replicas: 4
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 6Gi
  docsum-usvc:
    replicas: 1
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 4
        memory: 4Gi
  vllm_gaudi:
    replicas: 1
    # Note: For vllm_gaudi, the habana.ai/gaudi limit is set to the tensor-parallel-size value defined in deployment/pipelines/chatqa/resources-model-hpu.yaml
    hpa:
      minReplicas: 1
      maxReplicas: 4
      targetValue: 40m
      # average processing time of single token
      # 1/target value would mean tokens/sec
      # depends on used LLM model
      # metric used for scaling 'sum(rate(vllm:time_per_output_token_seconds_sum{service="vllm-gaudi-svc"}[1m])) by (<<.GroupBy>>) / (0.001 + sum(rate(vllm:time_per_output_token_seconds_count{service="vllm-gaudi-svc"}[1m])) by (<<.GroupBy>>))'
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 180
          policies:
          - type: Percent
            value: 25
            periodSeconds: 90
        scaleUp:
          selectPolicy: Max
          stabilizationWindowSeconds: 0
          policies:
          - type: Pods
            value: 1
            periodSeconds: 150
