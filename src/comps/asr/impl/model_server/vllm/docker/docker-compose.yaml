# Copyright (C) 2024-2026 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  asr-vllm-model-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: asr-vllm-audio:latest
    container_name: asr-vllm-model-server
    network_mode: host
    volumes:
      - "./data:/data"
    environment:
      HF_TOKEN: ${HF_TOKEN}
      HTTP_PROXY: ${HTTP_PROXY}
      HTTPS_PROXY: ${HTTPS_PROXY}
      NO_PROXY: ${NO_PROXY}
      VLLM_CPU_KVCACHE_SPACE: ${VLLM_CPU_KVCACHE_SPACE}
      VLLM_MAX_AUDIO_CLIP_FILESIZE_MB: 64
    ipc: host
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${ASR_VLLM_PORT}/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s
    command: "$ASR_VLLM_MODEL_NAME \
              --tensor-parallel-size $VLLM_TP_SIZE \
              --pipeline-parallel-size $VLLM_PP_SIZE \
              --dtype $VLLM_DTYPE --max-num-seqs $VLLM_MAX_NUM_SEQS \
              --max-model-len $VLLM_MAX_MODEL_LEN \
              --download_dir /data \
              --host 0.0.0.0 --port ${ASR_VLLM_PORT}"

  asr_usvc:
    build:
      context: ../../../../../../
      dockerfile: ./comps/asr/impl/microservice/Dockerfile
    container_name: asr-microservice
    ipc: host
    runtime: runc
    network_mode: host
    environment:
      HTTPS_PROXY: ${HTTPS_PROXY}
      HTTP_PROXY: ${HTTP_PROXY}
      LLM_MODEL_NAME: ${ASR_VLLM_MODEL_NAME}
      LLM_MODEL_SERVER: "vllm"
      LLM_MODEL_SERVER_ENDPOINT: http://localhost:${ASR_VLLM_PORT}
      NO_PROXY: ${NO_PROXY}
    restart: unless-stopped
    depends_on:
      asr-vllm-model-server:
        condition: service_healthy
