# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

## Provide your Hugging Face API key to enable access to Hugging Face models.
#HF_TOKEN=<your-hf-api-key>

## VLLM Model Server Settings ##
LLM_VLLM_MODEL_NAME="Intel/neural-chat-7b-v3-3"
LLM_VLLM_PORT=8008

#LLM_CONNECTOR="langchain" # Defaults to "generic" if not set. Options: "langchain", "generic".

## VLLM Settings ##
VLLM_CPU_KVCACHE_SPACE=40
VLLM_DTYPE=bfloat16
VLLM_MAX_NUM_SEQS=256
VLLM_SKIP_WARMUP=false
# set VLLM_TP_SIZE to number of sockets available on your system
# set VLLM_CPU_OMP_THREADS_BIND according to # of cores you are assigning to the vllm container
# optimal values for vllm is 48 threads running in container that has 48cores + 2 for vllm framework
# example output from lscpu:
# NUMA:
# NUMA node(s):           2
# NUMA node0 CPU(s):      0-59,120-179
# NUMA node1 CPU(s):      60-119,180-239
VLLM_TP_SIZE=1
OMP_NUM_THREADS=32
VLLM_PP_SIZE=1
VLLM_MAX_MODEL_LEN=4096

VLLM_OPENVINO_KVCACHE_SPACE=40
VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS="ON"
VLLM_OPENVINO_CPU_KV_CACHE_PRECISION="u8"

## Proxy Settings â€“ Uncomment if Needed ##
#NO_PROXY=<your-no-proxy>
#HTTP_PROXY=<your-http-proxy>
#HTTPS_PROXY=<your-https-proxy>

