# OPEA ERAG vLLM Model Server for Intel® Xeon® Processor

Part of the Intel® AI for Enterprise RAG (ERAG) ecosystem.

## 🔍 Overview

This vLLM Model Server enables LLM inference on CPU platforms using the [vLLM](https://github.com/vllm-project/vllm). Designed to run on Intel® Xeon® processors, it provides a robust serving endpoint ideal for CPU-centric environments or those with limited or unavailable GPU or HPU resources.

### Features

- Enables LLM serving relying on CPUs
- Utilizes Intel® Xeon® Processors features like AMX (Advanced Matrix Extensions) and AVX-512 for enhanced performance

## 🔗 Related Components

This service integrates with OPEA ERAG LLM Microservice.

## License

OPEA ERAG is licensed under the Apache License, Version 2.0.

Copyright © 2024–2025 Intel Corporation. All rights reserved.