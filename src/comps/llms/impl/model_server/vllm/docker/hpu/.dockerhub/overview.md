# OPEA ERAG vLLM Model Server for Intel® Gaudi® AI Accelerators

Part of the Intel® AI for Enterprise RAG (ERAG) ecosystem.

## 🔍 Overview

This vLLM Model Server delivers high-performance Large Language Model inference on Intel® Gaudi® AI Accelerators using the tailored vLLM backend, see [HabanaAI/vllm-fork](https://github.com/HabanaAI/vllm-fork.git). 

### Features

- Optimized for Intel® Gaudi® AI Accelerators with full hardware acceleration support
- Advanced features including FP8 quantization for improved performance

## 🔗 Related Components

This service integrates with OPEA ERAG LLM Microservice.

## License

OPEA ERAG is licensed under the Apache License, Version 2.0.

Copyright © 2024–2025 Intel Corporation. All rights reserved.