# OPEA ERAG vLLM Model Server for IntelÂ® GaudiÂ® AI Accelerators

Part of the IntelÂ® AI for Enterprise RAG (ERAG) ecosystem.

## ğŸ” Overview

This vLLM Model Server delivers high-performance Large Language Model inference on IntelÂ® GaudiÂ® AI Accelerators using the tailored vLLM backend, see [HabanaAI/vllm-fork](https://github.com/HabanaAI/vllm-fork.git). 

### Features

- Optimized for IntelÂ® GaudiÂ® AI Accelerators with full hardware acceleration support
- Advanced features including FP8 quantization for improved performance

## ğŸ”— Related Components

This service integrates with OPEA ERAG LLM Microservice.

## Disclaimer

This container image is intended for demo purposes only and not intended for production use.
To receive expanded security maintenance from Canonical on the Ubuntu base layer,
you may follow the [how-to guide to enable Ubuntu Pro in a Dockerfile](https://documentation.ubuntu.com/pro-client/en/docs/howtoguides/enable_in_dockerfile/) 
which will require the image to be rebuilt.

## License

OPEA ERAG is licensed under the Apache License, Version 2.0.

Copyright Â© 2024â€“2025 Intel Corporation. All rights reserved.