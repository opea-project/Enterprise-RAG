# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  vllm-fp8-quantization:
    container_name: vllm-fp8-quantization
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ${PWD}/${HUGGINGFACE_HUB_CACHE}:/data
    environment:
      - BATCH_SIZE=${BATCH_SIZE}
      - LLM_VLLM_MODEL_NAME=${LLM_VLLM_MODEL_NAME}
      - HF_TOKEN=${HF_TOKEN}
      # overridden in container to make sure hf libraries download/ access model from same location as vLLM server
      - HUGGINGFACE_HUB_CACHE=/data/hub
      - FP8_DATASET_PATH=${FP8_DATASET_PATH}
      - HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}
    runtime: habana
    cap_add:
      - SYS_NICE
    command: >
      /bin/bash -c "echo $LLM_VLLM_MODEL_NAME &&
      ./calibrate_model.sh -m $LLM_VLLM_MODEL_NAME -d $FP8_DATASET_PATH -t ${VLLM_TP_SIZE} -o /data/inc"
